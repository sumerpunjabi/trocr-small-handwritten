{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d357e1cc-c15a-4c33-a431-d256b6dc76cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: python-doctr 1.0.0 does not provide the extra 'torch'\n"
     ]
    }
   ],
   "source": [
    "%pip install \"python-doctr[torch]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f662d-6c88-423a-9d17-9f085c9d9980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from doctr.models import detection_predictor\n",
    "from doctr.io import DocumentFile\n",
    "import cv2 as cv\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import torch\n",
    "\n",
    "path = 'cheque.png'\n",
    "img = Image.open(path).convert('RGB')\n",
    "img = np.array(img)\n",
    "pages = DocumentFile.from_images([path])\n",
    "predictor = detection_predictor('db_resnet50', pretrained=True)\n",
    "out = predictor(pages)\n",
    "\n",
    "height, width, _ = img.shape\n",
    "for bbox in out[0]['words']:\n",
    "    x1 = int(bbox[0] * width)\n",
    "    y1 = int(bbox[1] * height)\n",
    "    x2 = int(bbox[2] * width)\n",
    "    y2 = int(bbox[3] * height)\n",
    "    cv.rectangle(img, (x1, y1), (x2, y2), (0,255,0), 1)\n",
    "\n",
    "img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "img = img[..., ::-1]\n",
    "img = Image.fromarray(img)\n",
    "\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"./models/trocr-small-handwritten\", local_files_only=True)\n",
    "# calling the processor is equivalent to calling the feature extractor\n",
    "pixel_values = processor(img, return_tensors=\"pt\").pixel_values\n",
    "print(pixel_values.shape)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"./models/trocr-small-handwritten\", local_files_only=True)\n",
    "\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e62438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "243053**\n",
      "GOOD\n",
      "28013:\n",
      "10\n",
      "MSA/115\n",
      "UCBA0000238\n",
      "TREASURR\n",
      "*\n",
      "SECRETARY\n",
      "SELATIVE.\n",
      "TAMILNADU\n",
      "059.\n",
      "600\n",
      "KVL-I-\n",
      "***\n",
      "600\n",
      "APPROVED\n",
      "059.\n",
      "X\n",
      "6\n",
      "ALLENMAN\n",
      "BANK02\n",
      "UC0\n",
      "OOR\n",
      "QTY\n",
      "23800100011938\n",
      "WELFARE\n",
      "ASSOCIATION\n",
      "KRISHNA\n",
      "FOR\n",
      "OWNERS\n",
      "APARTMENTS\n",
      "A/C.\n",
      "S.B.\n",
      "NO.\n",
      "INTLS\n",
      "LF\n",
      "@\n",
      "#.\n",
      "*5\n",
      "@.0\n",
      "QTR.#.\n",
      "STAT\n",
      "ON\n",
      "**\n",
      "BAKE/Y\n",
      "MAKE\n",
      "6493/\n",
      "R.RS.4\n",
      "RUPEES\n",
      "4\n",
      "S\n",
      "X\n",
      "5.9\n",
      "X\n",
      "AMOUNT\n",
      "SIX\n",
      "E\n",
      "OR\n",
      "SRICE\n",
      "BEARER\n",
      "RAT\n",
      "PAY\n",
      "SHRIVIVASAZ\n",
      "D\n",
      "***\n",
      ":\n",
      "DATE\n",
      "13/05/13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from doctr.models import detection_predictor\n",
    "from doctr.io import DocumentFile\n",
    "import cv2 as cv\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import torch\n",
    "\n",
    "path = 'cheque.png'\n",
    "img = Image.open(path).convert('RGB')\n",
    "img = np.array(img)\n",
    "pages = DocumentFile.from_images([path])\n",
    "predictor = detection_predictor('db_resnet50', pretrained=True)\n",
    "out = predictor(pages)\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"./models/trocr-small-printed\", local_files_only=True)\n",
    "device = torch.device('cpu')\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"./models/trocr-small-printed\", local_files_only=True)\n",
    "\n",
    "height, width, _ = img.shape\n",
    "for bbox in out[0]['words']:\n",
    "    # Calculate coordinates\n",
    "    x1 = int(bbox[0] * width)\n",
    "    y1 = int(bbox[1] * height)\n",
    "    x2 = int(bbox[2] * width)\n",
    "    y2 = int(bbox[3] * height)\n",
    "    \n",
    "    # Crop the image based on bounding box\n",
    "    # Ensure the crop is valid\n",
    "    if x2 > x1 and y2 > y1:\n",
    "        crop_img = img[y1:y2, x1:x2]\n",
    "        \n",
    "        # Convert numpy array back to PIL Image for TrOCR\n",
    "        pil_img = Image.fromarray(crop_img)\n",
    "\n",
    "        # Generate text for the cropped image\n",
    "        pixel_values = processor(pil_img, return_tensors=\"pt\").pixel_values\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
